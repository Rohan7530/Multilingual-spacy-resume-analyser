{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apmosys/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/apmosys/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/apmosys/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID                                         Resume_str  \\\n",
      "625   47067533           DIRECTOR, BUSINESS DEVELOPMENT       ...   \n",
      "2264  31273413           ARTS EDUCATOR       Summary     Creat...   \n",
      "2466  38663892           MANAGEMENT AND PROGRAM ANALYSIS      ...   \n",
      "1780  17488801           ENGINEERING MANAGER/QUALITY MANAGER  ...   \n",
      "1311  81508860           DIGITAL MARKETING DIRECTOR           ...   \n",
      "\n",
      "                                            Resume_html              Category  \n",
      "625   <div class=\"fontsize fontface vmargins hmargin...  BUSINESS-DEVELOPMENT  \n",
      "2264  <div class=\"fontsize fontface vmargins hmargin...                  ARTS  \n",
      "2466  <div class=\"fontsize fontface vmargins hmargin...              AVIATION  \n",
      "1780  <div class=\"fontsize fontface vmargins hmargin...           ENGINEERING  \n",
      "1311  <div class=\"fontsize fontface vmargins hmargin...         DIGITAL-MEDIA  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#loading the spacy model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m skill_pattern_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpacy Resume Analysis/jz_skill_patterns.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "#spacy\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#Visualization\n",
    "from spacy import displacy\n",
    "import pyLDAvis.gensim_models\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Data loading/ Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "\n",
    "#nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "\n",
    "#warning\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#here we will feed the entity ruler and also load the spacy model\n",
    "df = pd.read_csv(\"resumeScraper/Resume.csv\")\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data = df.copy().iloc[\n",
    "    0:100,\n",
    "]\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "#loading the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "skill_pattern_path = \"Spacy Resume Analysis/jz_skill_patterns.jsonl\"\n",
    "\n",
    "print(\"model loaded successfully\")\n",
    "\n",
    "#adding entity ruler to the pipeline\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path) #here it is getting the patterns from the jsonl file to load the entity ruler\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "#adding two functions to get the skills\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))\n",
    "\n",
    "#using nltk to clean the dataset\n",
    "clean = []\n",
    "for i in range(data.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        data[\"Resume_str\"].iloc[i],\n",
    "    )\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [\n",
    "        lm.lemmatize(word)\n",
    "        for word in review\n",
    "        if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)\n",
    "\n",
    "#this is the code for applying functions to the clean array which contains word related to the entity ruler\n",
    "# 1. create a column called \"clean resume\" and add the clean keywords in it as par the ner\n",
    "# 2. apply get skills function to the clean array to filterout words that contain the skills only\n",
    "# 3. only keep the unique skills in the \"clean resume\" column\n",
    "\n",
    "data[\"Clean_Resume\"] = clean\n",
    "data[\"skills\"] = data[\"Clean_Resume\"].str.lower().apply(get_skills)\n",
    "data[\"skills\"] = data[\"skills\"].apply(unique_skills)\n",
    "print(data.head())\n",
    "\n",
    "fig = px.histogram(\n",
    "    data, x=\"Category\", title=\"Distribution of Jobs Categories\"\n",
    ").update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()\n",
    "\n",
    "#creating a variable called job_cat to search regarding the job category\n",
    "Job_cat = data[\"Category\"].unique()\n",
    "Job_cat = np.append(Job_cat, \"ALL\")\n",
    "\n",
    "#this si the graph that shows skills in a single job category\n",
    "Job_Category = input(\"Enter job category (or 'ALL' for all categories): \")\n",
    "\n",
    "# Total_skills = []\n",
    "# if Job_Category != \"ALL\":\n",
    "#     fltr = data[data[\"Category\"] == Job_Category][\"skills\"]\n",
    "#     for x in fltr:\n",
    "#         for i in x:\n",
    "#             Total_skills.append(i)\n",
    "# else:\n",
    "#     fltr = data[\"skills\"]\n",
    "#     for x in fltr:\n",
    "#         for i in x:\n",
    "#             Total_skills.append(i)\n",
    "\n",
    "# fig = px.histogram(\n",
    "#     x=Total_skills,\n",
    "#     labels={\"x\": \"Skills\"},\n",
    "#     title=f\"{Job_Category} Distribution of Skills\",\n",
    "# ).update_xaxes(categoryorder=\"total descending\")\n",
    "# fig.show() \n",
    "# #uncomment the above code for the skills of job category defined\n",
    "\n",
    "# here we will build a wordcloud, which means the most words used in the job category\n",
    "text = \"\"\n",
    "for i in data[data[\"Category\"] == Job_Category][\"Clean_Resume\"].values:\n",
    "    text += i + \" \"\n",
    "# print(data[\"Category\"].unique()) #UNIT TEST PASSED\n",
    "# print(len(data[data[\"Category\"] == Job_Category])) #UNIT TEST PASSED\n",
    "# print(text) #UNIT TEST PASSED\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=900,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    min_font_size=6,\n",
    "    repeat=True,\n",
    "    mask=mask,\n",
    ")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.imshow(wc)\n",
    "\n",
    "plt.title(f\"Most Used Words in {Job_Category} Resume\", fontsize=15)\n",
    "plt.savefig(\"wordcloud.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sent = nlp(data[\"Resume_str\"].iloc[0])\n",
    "displacy.render(sent, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en-core-web-sm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for en-core-web-sm\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install en-core-web-sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
